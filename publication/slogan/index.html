<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.8.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Anaelia Ovalle, PhD">

  
  
  
    
  
  <meta name="description" content="Auditing machine learning-based (ML) healthcare tools for bias is critical to preventing patient harm, especially in communities that disproportionately face health inequities. General frameworks are becoming increasingly available to measure ML fairness gaps between groups. However, ML for health (ML4H) auditing principles call for a contextual, patient-centered approach to model assessment. Therefore, ML auditing tools must be (1) better aligned with ML4H auditing principles and (2) able to illuminate and characterize communities vulnerable to the most harm. To address this gap, we propose supplementing ML4H auditing frameworks with SLOGAN (patient Severity-based LOcal Group biAs detectioN), an automatic tool for capturing local biases in a clinical prediction task. SLOGAN adapts an existing tool, LOGAN (LOcal Group biAs detectioN), by contextualizing group bias detection in patient illness severity and past medical history. We investigate and compare SLOGAN&#39;s bias detection capabilities to LOGAN and other clustering techniques across patient subgroups in the MIMIC-III dataset. On average, SLOGAN identifies larger fairness disparities in over 75% of patient groups than LOGAN while maintaining clustering quality. Furthermore, in a diabetes case study, health disparity literature corroborates the characterizations of the most biased clusters identified by SLOGAN. Our results contribute to the broader discussion of how machine learning biases may perpetuate existing healthcare disparities.">

  
  <link rel="alternate" hreflang="en-us" href="/publication/slogan/">

  


  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0-1/css/all.min.css" integrity="sha256-4w9DunooKSr3MFXHXWyFER38WmPdm361bQS/2KUWZbU=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      
        
      

      
    
      

      
      

      
    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  




  


  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_hu079a575de87fa45ceb28f34db4fcd68e_1079249_32x32_fill_lanczos_center_2.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_hu079a575de87fa45ceb28f34db4fcd68e_1079249_192x192_fill_lanczos_center_2.png">

  <link rel="canonical" href="/publication/slogan/">

  
  
  
  
  
    
  
  
  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="">
  <meta property="og:url" content="/publication/slogan/">
  <meta property="og:title" content="Auditing Algorithmic Fairness in Machine Learning for Health with Severity-Based LOGAN | ">
  <meta property="og:description" content="Auditing machine learning-based (ML) healthcare tools for bias is critical to preventing patient harm, especially in communities that disproportionately face health inequities. General frameworks are becoming increasingly available to measure ML fairness gaps between groups. However, ML for health (ML4H) auditing principles call for a contextual, patient-centered approach to model assessment. Therefore, ML auditing tools must be (1) better aligned with ML4H auditing principles and (2) able to illuminate and characterize communities vulnerable to the most harm. To address this gap, we propose supplementing ML4H auditing frameworks with SLOGAN (patient Severity-based LOcal Group biAs detectioN), an automatic tool for capturing local biases in a clinical prediction task. SLOGAN adapts an existing tool, LOGAN (LOcal Group biAs detectioN), by contextualizing group bias detection in patient illness severity and past medical history. We investigate and compare SLOGAN&#39;s bias detection capabilities to LOGAN and other clustering techniques across patient subgroups in the MIMIC-III dataset. On average, SLOGAN identifies larger fairness disparities in over 75% of patient groups than LOGAN while maintaining clustering quality. Furthermore, in a diabetes case study, health disparity literature corroborates the characterizations of the most biased clusters identified by SLOGAN. Our results contribute to the broader discussion of how machine learning biases may perpetuate existing healthcare disparities."><meta property="og:image" content="/publication/slogan/featured.jpg">
  <meta property="twitter:image" content="/publication/slogan/featured.jpg"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2017-01-01T00:00:00&#43;00:00">
    
    <meta property="article:modified_time" content="2021-02-01T00:00:00&#43;00:00">
  

  


    









<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Article",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "/publication/slogan/"
  },
  "headline": "Auditing Algorithmic Fairness in Machine Learning for Health with Severity-Based LOGAN",
  
  "image": [
    "/publication/slogan/featured.jpg"
  ],
  
  "datePublished": "2017-01-01T00:00:00Z",
  "dateModified": "2021-02-01T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Anaelia Ovalle, PhD"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "",
    "logo": {
      "@type": "ImageObject",
      "url": "/images/icon_hu079a575de87fa45ceb28f34db4fcd68e_1079249_192x192_fill_lanczos_center_2.png"
    }
  },
  "description": "Auditing machine learning-based (ML) healthcare tools for bias is critical to preventing patient harm, especially in communities that disproportionately face health inequities. General frameworks are becoming increasingly available to measure ML fairness gaps between groups. However, ML for health (ML4H) auditing principles call for a contextual, patient-centered approach to model assessment. Therefore, ML auditing tools must be (1) better aligned with ML4H auditing principles and (2) able to illuminate and characterize communities vulnerable to the most harm. To address this gap, we propose supplementing ML4H auditing frameworks with SLOGAN (patient Severity-based LOcal Group biAs detectioN), an automatic tool for capturing local biases in a clinical prediction task. SLOGAN adapts an existing tool, LOGAN (LOcal Group biAs detectioN), by contextualizing group bias detection in patient illness severity and past medical history. We investigate and compare SLOGAN's bias detection capabilities to LOGAN and other clustering techniques across patient subgroups in the MIMIC-III dataset. On average, SLOGAN identifies larger fairness disparities in over 75% of patient groups than LOGAN while maintaining clustering quality. Furthermore, in a diabetes case study, health disparity literature corroborates the characterizations of the most biased clusters identified by SLOGAN. Our results contribute to the broader discussion of how machine learning biases may perpetuate existing healthcare disparities."
}
</script>

  

  


  


  


<link href="https://fonts.googleapis.com/css2?family=Plus+Jakarta+Sans:wght@300;400;500;600;700&display=swap" rel="stylesheet">
<style>
body {
  font-family: 'Plus Jakarta Sans', sans-serif !important;
}

 
h1, h2, h3, h4, h5, h6 {
  font-family: 'Plus Jakarta Sans', sans-serif !important;
  font-weight: 700;
  letter-spacing: -0.01em;
}

 
.navbar .nav-link {
  font-family: 'Plus Jakarta Sans', sans-serif !important;
  font-weight: 600;
}
</style>



 <style>
 
#publications .section-heading {
  order: -1;                 
  flex: 0 0 100%;
  max-width: 100%;
  margin-bottom: .75rem;
}

 
#publications .section-content {
  order: 0;
  flex: 0 0 100%;
  max-width: 100%;
}

 
#publications .row,
#publications .col-lg-8,
#publications .section-content {
  max-width: 100% !important;
  flex: 0 0 100% !important;
  width: 100% !important;
}

 
#publications .row {
  margin-left: 0 !important;
  margin-right: 0 !important;
  padding-left: 0 !important;
  padding-right: 0 !important;
}

 
#publications .pub-list {
  margin-top: 1rem;
}

 
#publications .article-title {
  font-weight: 600;
  letter-spacing: -0.01em;
}

#publications .article-metadata,
#publications .pub-abstract {
  font-size: 0.95rem;
  opacity: 0.9;
}


 
 
.pub-list .btn,
.pub-list .badge,
.pub-list .btn-primary,
.pub-list .btn-outline-primary {
  display: none !important;
}

 


</style>





<style>
 
.navbar .active,
.navbar .nav-item.active > a,
.navbar .nav-item > a.active,
.navbar .navbar-nav > li.active > a,
.navbar .navbar-nav > li > a.active {
  color: inherit !important;
  background: transparent !important;
  text-decoration: none !important;
  box-shadow: none !important;
}

 
.navbar .navbar-nav .nav-link:hover,
.navbar .navbar-nav > li > a:hover {
  color: #1a6aff !important;
  text-decoration: underline !important;
  text-decoration-thickness: 2px;
  text-underline-offset: 3px;
}
</style>


 <script>
document.addEventListener('DOMContentLoaded', function () {
  
  var nav = document.querySelector('.navbar');
  if (nav) {
    nav.querySelectorAll('.active').forEach(function(el){ el.classList.remove('active'); });
  }

  
  var body = document.body;
  if (body) {
    body.removeAttribute('data-spy');
    body.removeAttribute('data-target');
  }

  
  if (typeof window.jQuery !== 'undefined') {
    try { window.jQuery('body').scrollspy('dispose'); } catch (e) {}
  }
});
</script>



<style>
 

.avatar {
  width: 300px !important;
  height: 300px !important;
  object-fit: cover;  
  border-radius: 8px;
}
</style>





<style>
 
.navbar-main .navbar-nav > li > a,
.navbar .navbar-nav > li > a,
.navbar .navbar-nav .nav-link {
  font-size: 1.18rem !important;  
  font-weight: 600 !important;
  letter-spacing: -0.01em;
}

 
.navbar-main .navbar-nav > li,
.navbar .navbar-nav > li {
  padding: 0 0.4rem;
}
</style>


<style>
 

 
.navbar-main .navbar-brand,
.navbar .navbar-brand {
  display: inline-block;
  width: 28px;               
  height: 28px;              
  background: url("/img/logo.png") no-repeat center center / contain;
  text-indent: -9999px;      
  overflow: hidden;
  padding: 0 !important;     
  margin: 1 !important;
  line-height: 28px;
}

 

 
.navbar-main .navbar-brand,
.navbar .navbar-brand {
  transition: transform .08s ease;
}
.navbar-main .navbar-brand:hover,
.navbar .navbar-brand:hover {
  transform: translateY(-1px);
}
</style>



<style>
 
 
 
.widget {
  color: #000 !important;
} */

 
h1, h2, h3, h4, h5, h6,
.hero-title,
.section-title {
  color: #000 !important;
}

 

 

 
.hero-overlay {
  background: none !important;
}

 
.navbar {
  background: #fff !important;
}
</style>


  <title>Auditing Algorithmic Fairness in Machine Learning for Health with Severity-Based LOGAN | </title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  







<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>üëãüèΩ About</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#publications"><span>üî¨ Research</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/files/cv.pdf"><span>üìò CV</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>üì¨ Contact </span></a>
        </li>

        
        

        

        
        
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link  active" href="/"><span></span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      

      
      <li class="nav-item">
        <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
      </li>
      

      

    </ul>

  </div>
</nav>


  <div class="pub">

  












  

  
  
  
<div class="article-container pt-3">
  <h1>Auditing Algorithmic Fairness in Machine Learning for Health with Severity-Based LOGAN</h1>

  

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span><a href="/authors/admin/">Anaelia Ovalle, PhD</a></span>, <span><a href="/authors/sunipa-dev/">Sunipa Dev</a></span>, <span><a href="/authors/jieyu-zhao/">Jieyu Zhao</a></span>, <span><a href="/authors/majid-sarrafzadeh/">Majid Sarrafzadeh</a></span>, <span><a href="/authors/kai-wei-chang/">Kai-Wei Chang</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    February 2021
  </span>
  

  

  

  
  
  

  
  

</div>

    











  



<div class="btn-links mb-3">
  
  








  
    
  



<a class="btn btn-outline-primary my-1 mr-1" href="https://arxiv.org/pdf/2211.08742.pdf" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 js-cite-modal"
        data-filename="/publication/slogan/cite.bib">
  Cite
</button>















</div>


  
</div>



  <div class="article-container">

    
    <h3>Abstract</h3>
    <p class="pub-abstract">Auditing machine learning-based (ML) healthcare tools for bias is critical to preventing patient harm, especially in communities that disproportionately face health inequities. General frameworks are becoming increasingly available to measure ML fairness gaps between groups. However, ML for health (ML4H) auditing principles call for a contextual, patient-centered approach to model assessment. Therefore, ML auditing tools must be (1) better aligned with ML4H auditing principles and (2) able to illuminate and characterize communities vulnerable to the most harm. To address this gap, we propose supplementing ML4H auditing frameworks with SLOGAN (patient Severity-based LOcal Group biAs detectioN), an automatic tool for capturing local biases in a clinical prediction task. SLOGAN adapts an existing tool, LOGAN (LOcal Group biAs detectioN), by contextualizing group bias detection in patient illness severity and past medical history. We investigate and compare SLOGAN&rsquo;s bias detection capabilities to LOGAN and other clustering techniques across patient subgroups in the MIMIC-III dataset. On average, SLOGAN identifies larger fairness disparities in over 75% of patient groups than LOGAN while maintaining clustering quality. Furthermore, in a diabetes case study, health disparity literature corroborates the characterizations of the most biased clusters identified by SLOGAN. Our results contribute to the broader discussion of how machine learning biases may perpetuate existing healthcare disparities.</p>
    

    
    <div class="row">
      <div class="col-md-1"></div>
      <div class="col-md-10">
        <div class="row">
          <div class="col-12 col-md-3 pub-row-heading">Type</div>
          <div class="col-12 col-md-9">
            
            
            <a href="/publication/#2">
              Journal article
            </a>
            
          </div>
        </div>
      </div>
      <div class="col-md-1"></div>
    </div>
    <div class="d-md-none space-below"></div>
    

    
    <div class="row">
      <div class="col-md-1"></div>
      <div class="col-md-10">
        <div class="row">
          <div class="col-12 col-md-3 pub-row-heading">Publication</div>
          <div class="col-12 col-md-9">AAAI Health Intelligence</div>
        </div>
      </div>
      <div class="col-md-1"></div>
    </div>
    <div class="d-md-none space-below"></div>
    

    <div class="space-below"></div>

    <div class="article-style"><h1 id="div-classalert-alert-note"><div class="alert alert-note"></h1>
  <div>
    <h1 id="click-the-cite-button-above-to-demo-the-feature-to-enable-visitors-to-import-publication-metadata-into-their-reference-management-software">Click the <em>Cite</em> button above to demo the feature to enable visitors to import publication metadata into their reference management software.</h1>
<h1 id="heading"></h1>
  </div>
</div>
<h1 id="div-classalert-alert-note-1"><div class="alert alert-note"></h1>
  <div>
    <h1 id="click-the-slides-button-above-to-demo-academics-markdown-slides-feature">Click the <em>Slides</em> button above to demo Academic&rsquo;s Markdown slides feature.</h1>
<h1 id="heading"></h1>
  </div>
</div>
<h1 id="supplementary-notes-can-be-added-here-including-code-and-mathhttpssourcethemescomacademicdocswriting-markdown-latex">Supplementary notes can be added here, including 
<a href="https://sourcethemes.com/academic/docs/writing-markdown-latex/" target="_blank" rel="noopener">code and math</a>.</h1>
</div>

    





<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/source-themes/">Source Themes</a>
  
</div>



<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=/publication/slogan/&amp;text=Auditing%20Algorithmic%20Fairness%20in%20Machine%20Learning%20for%20Health%20with%20Severity-Based%20LOGAN" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=/publication/slogan/&amp;t=Auditing%20Algorithmic%20Fairness%20in%20Machine%20Learning%20for%20Health%20with%20Severity-Based%20LOGAN" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Auditing%20Algorithmic%20Fairness%20in%20Machine%20Learning%20for%20Health%20with%20Severity-Based%20LOGAN&amp;body=/publication/slogan/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=/publication/slogan/&amp;title=Auditing%20Algorithmic%20Fairness%20in%20Machine%20Learning%20for%20Health%20with%20Severity-Based%20LOGAN" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://web.whatsapp.com/send?text=Auditing%20Algorithmic%20Fairness%20in%20Machine%20Learning%20for%20Health%20with%20Severity-Based%20LOGAN%20/publication/slogan/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=/publication/slogan/&amp;title=Auditing%20Algorithmic%20Fairness%20in%20Machine%20Learning%20for%20Health%20with%20Severity-Based%20LOGAN" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>












  
    
    





  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <img class="avatar mr-3 avatar-square" src="/authors/admin/avatar_hu07fb0a5dbbc24254122f95fc82d30f8d_170482_270x270_fill_q90_lanczos_center.jpg" alt="Avatar">
    

    <div class="media-body">
      <h5 class="card-title"><a href="/">Anaelia Ovalle, PhD</a></h5>
      
      <p class="card-text">My research interests include algorithmic fairness, AI ethics, and trustworthy ML.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="/#contact" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/ovalle_elia" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/anaeliaovalle" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.com/citations?hl=en&amp;user=BLB0ybwAAAAJ&amp;view_op=list_works&amp;sortby=pubdate" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>


  
    
    





  
  
  
  
  
  <div class="media author-card content-widget-hr">
    

    <div class="media-body">
      <h5 class="card-title"><a href="/authors/sunipa-dev/"></a></h5>
      
      
      <ul class="network-icon" aria-hidden="true">
  
</ul>

    </div>
  </div>


  
    
    





  
  
  
  
  
  <div class="media author-card content-widget-hr">
    

    <div class="media-body">
      <h5 class="card-title"><a href="/authors/jieyu-zhao/"></a></h5>
      
      
      <ul class="network-icon" aria-hidden="true">
  
</ul>

    </div>
  </div>


  
    
    





  
  
  
  
  
  <div class="media author-card content-widget-hr">
    

    <div class="media-body">
      <h5 class="card-title"><a href="/authors/majid-sarrafzadeh/"></a></h5>
      
      
      <ul class="network-icon" aria-hidden="true">
  
</ul>

    </div>
  </div>


  
    
    





  
  
  
  
  
  <div class="media author-card content-widget-hr">
    

    <div class="media-body">
      <h5 class="card-title"><a href="/authors/kai-wei-chang/"></a></h5>
      
      
      <ul class="network-icon" aria-hidden="true">
  
</ul>

    </div>
  </div>


  










  
  
  <div class="article-widget content-widget-hr">
    <h3>Related</h3>
    <ul>
      
      <li><a href="/publication/harms/">Harms of gender exclusivity and challenges in non-binary representation in language technologies</a></li>
      
      <li><a href="/publication/robustness/">Improving Adversarial Robustness to Sensitivity and Invariance Attacks with Deep Metric Learning</a></li>
      
      <li><a href="/publication/jmir-2021/">Leveraging Social Media Activity and Machine Learning for HIV and Substance Abuse Risk Assessment</a></li>
      
      <li><a href="/publication_template/conference-paper/">An example conference paper</a></li>
      
      <li><a href="/publication_template/journal-article/">An example journal article</a></li>
      
    </ul>
  </div>
  



  </div>
</div>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js" integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/languages/r.min.js"></script>
        
      

    

    
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    <script>const isSiteThemeDark = false;</script>
    

    

    
    

    

    
    

    
    

    
    

    
    
    
    
    
    
    
    
    
    
    
    
    <script src="/js/academic.min.738eb6c268ee4959345a9f0efc01a6e9.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
